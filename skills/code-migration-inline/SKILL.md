---
name: code-migration
description: Migrate applications between technologies using kantra static analysis and automated fixes. Use when migrating Java, Node.js, Python, Go, or .NET applications. Keywords: kantra, migration, upgrade, modernize.
---

# Code Migration

Migrate applications by identifying issues from multiple sources, fixing them systematically, and validating the result.

## Issue Sources

Collect issues from ALL of these sources during analysis.

| Source | Examples |
|--------|----------|
| Kantra analysis | Deprecated APIs, breaking changes, migration patterns |
| Build errors | Compilation failures, type errors, missing deps |
| Lint errors | Style violations, unused imports |
| Test failures | Broken tests from API changes |
| Target docs | Breaking changes Kantra doesn't detect (check `targets/<target>.md`) |

Kantra is a static source code analysis tool that uses rules to identify migration issues in the source code.

---

## Phase 1: Discovery

### Step 1: Explore Project Structure

Discover build system, test commands, and lint configuration.

**Find build system:**

| File Found | Build Command |
|------------|---------------|
| `package.json` | `npm run build` or `yarn build` |
| `pom.xml` | `mvn compile` or `mvn package` |
| `build.gradle` / `build.gradle.kts` | `./gradlew build` |
| `Makefile` | `make` |
| `go.mod` | `go build ./...` |
| `Cargo.toml` | `cargo build` |
| `*.csproj` / `*.sln` | `dotnet build` |

**Find test commands:**

| Test Type | How to Find |
|-----------|-------------|
| Unit tests | Check `package.json` scripts for `test`, `test:unit`; or `mvn test`, `go test ./...` |
| Integration | Look for `test:integration`, `test:e2e` scripts |
| E2E | Look for Cypress (`cypress run`), Playwright (`npx playwright test`), or similar |

**Find lint command:**

| Tool | Detection |
|------|-----------|
| ESLint | `.eslintrc*` file → `npm run lint` or `npx eslint .` |
| Prettier | `.prettierrc*` file → `npx prettier --check .` |
| Go | `golangci-lint run` |
| Python | `flake8`, `pylint`, `ruff` |

**Find dev server command:**

Look for how to run the application locally (e.g., `npm start`, `npm run dev`).

**Record findings:**

```
Build: <command>
Dev server: <command>
Lint: <command>
Unit tests: <command>
Integration tests: <command>
E2E tests: <command>
Primary language: <language>
```

### Step 2: Build Kantra Command

Construct the Kantra analyze command flags.

**Ask user:**
- Use custom rules? (If yes, get path)
- Enable default rulesets?

**Detect provider:**

| Files Found | Provider |
|-------------|----------|
| `*.java`, `pom.xml`, `build.gradle` | `java` |
| `*.ts`, `*.tsx`, `package.json` with TS deps | `typescript` |
| `*.js`, `*.jsx`, `package.json` | `javascript` |
| `go.mod`, `*.go` | `go` |
| `*.py`, `requirements.txt`, `pyproject.toml` | `python` |
| `*.cs`, `*.csproj` | `dotnet` |

**Build flags:**

```bash
# Base flags
--provider=<detected_provider>

# If custom rules provided:
--rules=<path_to_rules>

# If default rulesets enabled (and available for target):
--target=<migration_target>
```

**You add `--input` and `--output`:**

```bash
kantra analyze --input <project> --output $WORK_DIR/round-N/kantra <FLAGS>
```

### Step 3: Create Workspace

```bash
WORK_DIR=$(mktemp -d -t migration-$(date +%m_%d_%y_%H))
```

### Step 4: Check Target Technology Specific Guidance

Read `targets/<target>.md` if it exists. Follow pre-migration steps before Phase 2.

---

## Phase 2: Fix Loop

### First Round Only

Run initial analysis to create the fix plan:

1. Run Kantra: `kantra analyze --input <project> --output $WORK_DIR/round-1/kantra <FLAGS>`
2. Parse Kantra output using the helper script:
   - Overview: `python3 scripts/kantra_output_helper.py analyze $WORK_DIR/round-1/kantra/output.yaml`
   - File details: `python3 scripts/kantra_output_helper.py file $WORK_DIR/round-1/kantra/output.yaml <file>`
3. Run build and lint commands
4. Run unit tests
5. Collect ALL issues from ALL sources (see Issue Sources table)
6. Create `$WORK_DIR/status.md` using the template below

### Fix Loop Template

Create `$WORK_DIR/status.md`:

```markdown
# Migration Status

## Groups

- [ ] Group 1: [Name] - [Brief description]
- [ ] Group 2: [Name] - [Brief description]
- [ ] Group 3: [Name] - [Brief description]

## Group Details

### Group 1: [Name]
**Why grouped**: [Related issues, same subsystem, etc.]
**Issues**:
- [Issue from Kantra/build/lint/tests]
- [Issue from Kantra/build/lint/tests]
**Files**: [file1.ts, file2.ts]

### Group 2: [Name]
...

## Round Log

(Append after each round)
```

### Each Round

```
Round Checklist:
- [ ] Pick next incomplete group
- [ ] Apply fixes for that group
- [ ] Run Kantra + build + lint + unit tests
- [ ] Mark group complete in status.md
- [ ] Add new issues to plan if any appeared
```

1. **Pick**: Select first incomplete group from status.md
2. **Fix**: Apply all fixes for that group
3. **Validate**: Run Kantra, build, lint, unit tests
4. **Update**: Mark group done, log the round

**Run tests concisely:**
- Capture output, report only failures
- Format: `PASS: X tests, FAIL: Y tests`
- For failures, show test name and error message only

Append to status.md:
```markdown
### Round N: [Group Name]
- Fixed: [count] issues
- New issues: [count or "none"]
- Build: PASS/FAIL
- Tests: PASS/FAIL/NONE
```

### Exit Check

After each round, check:

| Condition | Done? |
|-----------|-------|
| All groups complete | ☐ |
| Kantra: 0 issues | ☐ |
| Build: passes | ☐ |
| Unit tests: pass | ☐ |

- **Any unchecked** → Continue loop (next group)
- **All checked** → Proceed to Phase 3

### If Stuck (Same Issue 3+ Rounds)

When an issue persists across 3+ rounds, analyze it:

**Run persistent issues script:**
```bash
python3 scripts/persistent_issues_analyzer.py $WORK_DIR
```

**For each persistent issue, determine:**

| Question | Check |
|----------|-------|
| False positive? | Rule too strict? Pattern actually valid? |
| Fixable? | Multiple approaches failed? Needs manual decision? |
| Blocking factor? | External deps? Domain knowledge needed? |

**Categorize:**
- **Fix**: Real issue, try different approach
- **Ignore**: False positive, document why in status.md
- **Document**: Real but needs manual intervention, add to status.md

---

## Phase 3: Final Validation

Run E2E/behavioral tests and complete target-specific validation.

### E2E Testing

1. Run E2E test command discovered in Phase 1
2. Report results concisely (pass/fail counts, failure details)
3. If tests FAIL → Fix issues, re-run
4. If tests PASS → Continue to target validation

### Target Validation

**Follow all post-migration steps in `targets/<target>.md`. These steps are mandatory — do not skip them.** The migration is not complete until all post-migration validation passes.

### Exit Criteria

All must be checked:

- [ ] Kantra: 0 issues
- [ ] Build: passes
- [ ] Unit tests: pass
- [ ] E2E tests: pass
- [ ] Target-specific validation complete

Update status.md:
```markdown
## Complete

- Total rounds: N
- Build: PASS
- Unit tests: PASS
- E2E tests: PASS
- Target validation: PASS
```

---

## Phase 4: Report

Before generating the report, write the final `## Action Required` section to status.md. This must reflect the **end state** of the migration, including visual fixes.

1. Read `$WORK_DIR/visual-diff-report.md` — check for any unchecked (`[ ]`) issues that remain after the visual fix loop
2. Read `$WORK_DIR/visual-fixes.md` — understand what visual issues were fixed and how
3. Remove any `visual_review` items from Action Required that were resolved by the visual-fix agent (i.e., the corresponding issues are now `[x]` in the diff report)
4. Add any **new** items discovered during visual fixing that need user attention (e.g., unfixable visual differences)

Append the final `## Action Required` section to status.md listing **every item the user should still review**. Use bullet format with type prefix:

```markdown
## Action Required

- **Unresolved Issue**: [description] → [recommendation]
- **False Positive**: [description] → [recommendation]
- **Visual Review** (page: [name]): [description] → [recommendation]
- **Manual Intervention**: [description] → [recommendation]
```

If nothing requires review:

```markdown
## Action Required

None
```

### 1. Read status.md

Read `$WORK_DIR/status.md` and extract:

- **Complete section**: total rounds, build/test/validation status
- **Groups**: list of groups with their completion status
- **Group Details**: for each group — name, description, files, issues
- **Round Log**: each round's fixed count, new issues, build/test results
- **Action Required section**: items the user must review (unresolved issues, false positives, visual reviews, manual interventions)

### 2. Read Kantra Assessment

Check for the latest Kantra output directory (`$WORK_DIR/round-*/kantra/output.yaml`). If a final round exists, note any residual incidents (rule, count, reason for keeping).

If no Kantra output exists, set `kantra_residual.total_incidents` to 0.

### 3. Read Visual Comparison Report

If `$WORK_DIR/visual-diff-report.md` exists, extract per-page results (page name, status, notes). Map unchecked items (`[ ]`) to `fail` status and checked items (`[x]`) to `pass`.

### 4. List Screenshot Directories

Check for `$WORK_DIR/baseline/` and `$WORK_DIR/post-migration/` directories. List filenames in each to populate the visual pages array.

If neither directory exists, set `visual.has_screenshots` to `false`.

### 5. Build report-data.json

Create `$WORK_DIR/report-data.json` using the following schema:

```json
{
  "migration": {
    "source": "string",
    "target": "string",
    "project": "string (project path)",
    "timestamp": "ISO 8601",
    "workspace": "string (workspace path)"
  },
  "summary": {
    "total_rounds": "number",
    "status": "complete|incomplete",
    "build": "PASS|FAIL",
    "unit_tests": "PASS|FAIL|NONE",
    "e2e_tests": "PASS|FAIL|NONE",
    "lint": "PASS|FAIL|NONE",
    "target_validation": "PASS|FAIL|NONE"
  },
  "action_required": [
    {
      "type": "unresolved_issue|false_positive|visual_review|manual_intervention",
      "description": "string",
      "recommendation": "string (optional)",
      "details": "string (optional)",
      "page": "string (optional, for visual_review)"
    }
  ],
  "groups": [
    {
      "name": "string",
      "status": "complete|incomplete",
      "issues_fixed": "number",
      "files": ["string"],
      "description": "string"
    }
  ],
  "rounds": [
    {
      "number": "number",
      "group": "string",
      "issues_fixed": "number",
      "new_issues": "number",
      "build": "PASS|FAIL",
      "tests": "string (e.g. '265/265' or '225/262 (37 snapshot mismatches)')"
    }
  ],
  "visual": {
    "has_screenshots": "boolean",
    "baseline_dir": "string (relative to work_dir)",
    "post_migration_dir": "string (relative to work_dir)",
    "pages": [
      {
        "name": "string",
        "baseline": "string (filename, e.g. 'login.png')",
        "post_migration": "string (filename, e.g. 'login.png')",
        "status": "pass|fail|info",
        "notes": "string"
      }
    ]
  },
  "kantra_residual": {
    "total_incidents": "number",
    "categories": [
      {
        "rule": "string",
        "count": "number",
        "reason": "string"
      }
    ]
  }
}
```

**Field population rules**:

- `migration.source` / `migration.target`: from the source and target technologies
- `migration.project`: the project path
- `migration.timestamp`: current time in ISO 8601
- `migration.workspace`: `$WORK_DIR`
- `summary`: from the Complete section of status.md
- `action_required`: from the Action Required section of status.md. Parse each bullet into type, description, and recommendation. If "None", use an empty array.
- `groups`: from Groups and Group Details sections. Mark `[x]` groups as "complete", `[ ]` as "incomplete". Count issues from Group Details. Extract files list.
- `rounds`: from Round Log entries. Parse fixed count, new issues count, build and test results.
- `visual`: from screenshot directories and visual-diff-report.md. If no screenshots exist, set `has_screenshots` to false and omit pages.
- `kantra_residual`: from the latest Kantra output. If 0 residual issues, set `total_incidents` to 0 and `categories` to empty array.

### 6. Read Visual Fixes

If `$WORK_DIR/visual-fixes.md` exists, read it to understand what visual issues were fixed and how. Use this to verify consistency of `action_required`:

- If a `visual_review` item in `action_required` refers to an issue that is now `[x]` in `visual-diff-report.md`, **remove it** from `action_required` — it was fixed.
- If `visual-diff-report.md` has unchecked (`[ ]`) issues that are **not** represented in `action_required`, **add them** as `visual_review` items.
- If `visual-fixes.md` documents fixes that contradict notes in `action_required` (e.g., an item says "not fixable" but `visual-fixes.md` shows it was fixed), update accordingly.

### 7. Verify Consistency

Before writing `report-data.json`, cross-check the data:

- `summary.status` should be `complete` only if all groups are complete, build passes, and tests pass
- `action_required` should not contain items that are contradicted by other artifacts (e.g., visual issues marked as needing review but already `[x]` in the diff report)
- `visual.pages` status values should match `visual-diff-report.md` — `[x]` → `pass`, `[ ]` → `fail`
- Every screenshot file referenced in `visual.pages` should exist in the baseline and post-migration directories

### 8. Generate HTML Report

Run:
```bash
python3 scripts/generate_migration_report.py $WORK_DIR
```

Tell the user the path to the generated `report.html`.

---

## Guidelines

- **One group per round** for clear feedback
- **Follow planned order** - foundation before dependent changes
- **Verify each fix** - don't break existing features
- **Document unfixable issues** after 2+ failed approaches
- **Use all issue sources** - Kantra is just one input
- **Report test results concisely** - counts and failures only, not full output
